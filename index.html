<!DOCTYPE html>
    <html lang="en">
      <head>
<meta charset="utf-8" />

<title>Causality for Machine Learning</title>
<meta name="description" content="An online research report on causality for machine learning by Cloudera Fast Forward." />

<meta property="og:title" content="Causality for Machine Learning" /> 
<meta property="og:description" content="An online research report on causality for machine learning by Cloudera Fast Forward." />
<meta property="og:image" content="https://ff13.fastforwardlabs.com/causality.png" />
<meta property="og:url" content="https://ff13.fastforwardlabs.com" />
<meta name="twitter:card" content="summary_large_image" />

<meta name="viewport" content="width=device-width" />
<link rel="icon" type="image/x-icon" href="favicon.ico" />

<style type="text/css">
    
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Bold.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Bold.woff') format('woff');
    font-weight: bold;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-BoldItalic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-BoldItalic.woff') format('woff');
    font-weight: bold;
    font-style: italic;
  }
  
    * {
      box-sizing: border-box;
    }
    html {
      background: #fff;
      font-family: "Plex Sans", serif, sans-serif;
      font-size: 17.5px;
      line-height: 28px;
    }
    body {
      margin: 0;
    }
    .content {
      max-width: 64ch;
      padding-left: 2ch;
      padding-right: 2ch;
      margin: 0 auto;
      display: block;
      padding-bottom: 0px;
    }
   p, ul, ol {
      margin: 0;
    }
    ul, ol {
      padding-left: 3ch;
    }
  p {
   // text-indent: 3ch;
}
    li p:first-child {
      text-indent: 0;
    }

    #pdf-logo {
      display: none;
    }

   hr {
      margin: 0;
      border-top-color: black;
      margin-top: -0.5px;
      margin-bottom: 27.5px;
    }
  
h1, h2, h3, h4, h5, h6, button { font-size: inherit; line-height: inherit; font-style: inherit; font-weight: inherit; margin: 0; font-feature-settings: "tnum"; border: none; background: transparent; padding: 0;  }
button:focus, button:hover {
  background: rgba(0,0,0,0.125);
  outline: none;
}
h1 {
  font-size: 42px;
  line-height: 56px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h2 {
  font-size: 31.5px;
  line-height: 42px;
  font-weight: bold;
  margin-top: 28px;
  margin-bottom: 14px;
}
h3 {
  font-size: 26.25px;
  line-height: 35px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h4 {
  font-size: 21px;
  line-height: 28px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h5 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-weight: bold;
}
h6 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-style: italic;
}
p {
  margin-bottom: 14px;
}
.content {
  position: relative;
  }
figure {
  margin: 0;
  margin-top: 14px;
  margin-bottom: 28px;
  display: block;
  position: relative;
  page-break-inside: avoid;
}
blockquote {
  margin: 0;
   margin-top: 14px;
  margin-bottom: 14px;
margin-left: 2ch;
}
blockquote + blockquote {
  margin-top: 0;
}
figcaption {
  font-family: "Plex Mono", serif, monospace;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 21px;
}
.info {
  background: #efefef;
  padding-left: 2ch;
  padding-right: 2ch;
  padding-top: 14px;
  padding-bottom: 14px;
  margin-bottom: 28px;
}
.info p:last-child {
  margin-bottom: 0;
}
img {
  display: block;
  position: relative;
  max-width: 100%;
  margin: 0 auto;
  page-break-inside: avoid;
}
code {
  font-size: 0.9em;
  line-height: 1.2;
  background: rgba(0,0,0,0.125);
  padding: 0 0.3em;
}
pre {
  font-size: 0.9em;
  line-height: 1.2;
  background: rgba(0,0,0,0.125);
  overflow-x: scroll;
  max-width: 100%;
  padding-left: 1ch;
  padding-right: 1ch;
  padding-top:0.625em;
  padding-bottom:0.625em;
}
pre code {
  background: transparent;
}

table {
  min-width: 100%;
  text-align: left;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 18.900000000000002px;
  border-collapse: collapse;
}
table, th, td {
  border: solid 1px black;
}
td {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  valign: top;
  vertical-align: top;
}
th {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  vertical-align: top;
  background: #efefef;
}
table ul, table ol {
  list-style-position: inside;
  padding-left: 0;
}

  a {
    color: inherit;
  }
  .table-of-contents {
    background: #efefef;
    position: fixed;
    left: 0;
    top: 0;
    width: 32ch;
    height: 100vh;
    overflow-y: auto;
    background: #efefef;
      // background: rgba(230,230,230,0.85);
      //   backdrop-filter: blur(5px);
  }
  body {
    padding-left: 32ch;
  }
  p:empty {
    display: none;
  }
  ul, ol {
  margin-bottom: 14px;
  }

  #report-iso {
    display: none;
  }

.table-of-contents {
    counter-reset: chapters;
}
 .table-of-contents ul {
    list-style: none;
    padding-left: 0;
    margin-bottom: none;
  }
 .table-of-contents > ul {
    padding-bottom: 28px;
  }
  .table-of-contents > ul > li > a:before {
          counter-increment: chapters;
          content: counter(chapters) ". ";
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }

 .table-of-contents > ul > li > ul > li {
    font-weight: normal;
    font-style: normal;
    text-transform: none;
    letter-spacing: 0;
    margin-left: 0;
  }
 .table-of-contents > ul > li > ul > li > ul > li {
    font-weight: normal;
    font-style: italic;
  }
 .table-of-contents a {
    text-decoration: none;
  }
  .table-of-contents a:hover {
    text-decoration: underline;
  }
 sup {
  }
  .table-of-contents ul a {
    display: block;
    padding-left: 3ch;
    text-indent: -1ch;
    padding-right: 2ch;
  }
  .table-of-contents ul li a.active {
    position: relative;
    background: #ddd;
    // text-decoration: line-through;
  }

 .table-of-contents > ul > li > ul > li > a {
    font-size: 15.75px;
      line-height: 25.2px;
    // padding-left: 4ch;
  }
  .table-of-contents > ul > li > ul > li > ul > li > a {
    padding-left: 5ch;
  }

h1 {
    counter-reset: chp;
}
h2 {
  position: relative;
  display: block;
  page-break-before: always;
  padding-top: 42px;
}
  h2:before {
    position: absolute;
    left: 0;
    top: 0;
      font-size: 17.5px;
    color: black;
    counter-increment: chp;
    content: "chapter " counter(chp);
    text-transform: uppercase;
  }

  .toc-desktop-hidden .table-of-contents {
    width: auto;
  }
  .toc-desktop-hidden #contents-label {
    display: none;
  }
  .toc-desktop-hidden .table-of-contents ul {
    display: none;
  }
  body.toc-desktop-hidden {
    padding-left: 5ch;
  }
  body:before {
    content: " ";
    height: 28px;
    width: 96ch;
    background: black;
    position: absolute;
    left: 0;
    top: 0;
    z-index: 999;
    display: none;
  }
    #toc-header {
      margin-top: 14px;
      margin-bottom: 14px;
      margin-left: 1ch;
      margin-right: 1ch;
    }

  @media screen and (max-width: 1028px) {
    h1 {
      font-size: 36.75px;
      line-height: 49px;
      font-weight: bold;
      margin-top: 14px;
      margin-bottom: 14px;
    }
    .table-of-contents ul li {
      padding-top: 3.5px;
      padding-bottom: 3.5px;
    }

    #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
    }

    body {
      padding-left: 0;
      padding-top: 42px;
    }
    .content {
        overflow-wrap: break-word;
        word-wrap: break-word;
    }
    #contents-label {
      display: none;
    }
    .table-of-contents {
      height: auto;
      width: 100%;
      z-index: 3;
    }
  body.toc-mobile-show .content:before {
      content: "";
      position: fixed;
      left: 0;
      top: 0;
      bottom: 0;
      right: 0;
      background: rgba(0,0,0,0.25);
      z-index: 2;
      border-top: solid 42px #aaa;
    }

    .table-of-contents > ul {
      display: none;
    }
   body.toc-mobile-show {
      overflow: hidden;
    }
    body.toc-mobile-show #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
      position: relative;
    }
    body.toc-mobile-show .table-of-contents {
      width: 32ch;
      height: 100vh;
      max-width: calc(100% - 4ch);
      overflow: auto;
    }
   body.toc-mobile-show .table-of-contents > ul {
      display: block;
      padding-bottom: 28px;
      position: relative;
    }
    body.toc-mobile-show #contents-label {
      display: inline;
      position: relative;
    }
  }
}
</style>
<script>
    function inViewport(elem) {
      let bounding = elem.getBoundingClientRect();
      return (
        bounding.top >= 0 &&
        bounding.left >= 0 &&
        bounding.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&
        bounding.right <= (window.innerWidth || document.documentElement.clientWidth)
      );
    };

    function setActive(target_id) {
      let selector = '.table-of-contents ul li a[href="#' + target_id + '"]'
      let link = document.querySelector(selector)
      if (link !== null) {
        link.className = 'active'
      }
    }

    window.addEventListener("load", (event) => {
      let headings = document.querySelectorAll('h2, h3');
      let links = document.querySelectorAll('.table-of-contents ul li a')

      observer = new IntersectionObserver((entry, observer) => {
        if (entry[0].intersectionRatio === 1) {
          for (let link of links) {
            link.className = ''
          }
          let target_id = entry[0].target.getAttribute('id')
          setActive(target_id)
        }
      }, { threshold: 1, rootMargin: "0px 0px -50% 0px" });

      let first = true
      for (let heading of headings) {
        if (first && inViewport(heading)) {
          setActive(heading.getAttribute('id'))
          first = false
        }
        observer.observe(heading);
      }

      document.querySelector('#toggle_contents').addEventListener('click', () => {
        let body = document.body
        if (window.innerWidth > 1027) {
          let hidden_class = "toc-desktop-hidden"
          if (body.className === hidden_class) {
            body.className = ''
          } else {
            body.className = hidden_class
          }
        } else {
          let show_class = "toc-mobile-show"
          if (body.className === show_class) {
            body.className = ''
          } else {
            body.className = show_class
          }
        }
      })

      for (let link of links) {
        link.addEventListener('click', (e) => {
          let href = e.target.getAttribute('href')
          let elem = document.querySelector(href)
          window.scroll({
            top: elem.offsetTop - 28,
            left: 0,
            behavior: 'smooth'
          })
          if (window.innerWidth < 1028) {
            document.body.className = ''
          }
          e.preventDefault() 
        })
      }

      document.querySelector('.content').addEventListener('click', () => {
        if (window.innerWidth < 1028) {
          document.body.className = ''
        }
      })
      document.querySelector('.table-of-contents').addEventListener('click', (e) => {
        e.stopPropagation()
      })

      let mediaQueryList = window.matchMedia("(max-width: 1028px)");
      function handleBreakpoint(mql) {
        // clear any left over toggle classes
        document.body.className = ''
      }
      mediaQueryList.addListener(handleBreakpoint);
    }, false);
  </script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-157475426-6', 'auto');
  ga('send', 'pageview');

  window.addEventListener('load', function() {
    document.getElementById('report-pdf-download').addEventListener('click', function() {
      ga('send', {
        hitType: 'pageview',
        page: '//FF13-Causality_for_Machine_Learning-Cloudera_Fast_Forward.pdf'
      });
    });
  })

</script>
<!-- End Google Analytics -->
</head>
      <body>
        <div class="content" style="position: relative;">
          <div id="html-logo" style="margin-top: 28px; line-height: 0; display: flex;">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html"><img alt="Cloudera Fast Forward" style="display: block; height: 14px; margin-bottom: 7px;" src='/figures/cloudera-fast-forward-logo.png' /></a>
          </div>
          <div id="pdf-logo" style="margin-top: 28px; ">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward</a>
          </div>
          <h1 id="meta-learning">Meta-Learning</h1>
<p>FF15 · <em>September 2020</em></p>
<figure><img src="figures/ff13-cover-splash.png" alt="Causality for Machine Learning report cover"><figcaption>Causality for Machine Learning report cover</figcaption></figure>
<p><em>This is an applied research report by <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward</a>. We write reports about emerging technologies. Accompanying each report are working prototypes that exhibit the capabilities of the algorithm and offer detailed technical advice on its practical application. Read our full report on causality for machine learning below or <a href="/FF13-Causality_for_Machine_Learning-Cloudera_Fast_Forward.pdf" target="_blank" id="report-pdf-download">download the PDF</a>. Also be sure to check out the complementary prototype, <a href="https://scene.fastforwardlabs.com">Scene</a>.</em></p>
<p><div class="table-of-contents"><div id="toc-header" style="display: flex; font-weight: bold; text-transform: uppercase;">
     <div><button id="toggle_contents" style="padding-left: 0.5ch; padding-right: 0.5ch; cursor: pointer; position: relative; top: -1px;">☰</button><span id="contents-label" style="margin-left: 0;"> Contents</span></div>
  </div><ul><li><a href="#introduction">Introduction</a><ul><li><a href="#why-should-we-care%3F">Why should we care?</a></li><li><a href="#why-now%3F">Why now?</a></li></ul></li><li><a href="#framing-the-problem">Framing the problem</a></li><li><a href="#solving-the-problem">Solving the problem</a><ul><li><a href="#data-set-up">Data set-up</a></li><li><a href="#meta-learning">Meta-learning</a></li><li><a href="#model-agnostic-meta-learning-(maml)">Model Agnostic Meta-learning (MAML)</a></li></ul></li></ul></div></p>
<p>In early spring of 2019, we researched approaches that would allow a machine learning practitioner to perform supervised learning with only a
limited number of examples available during training. This search led us to a new paradigm: meta-learning, in which an algorithm not only learns
from a handful of examples, but also learns to classify novel classes during model inference. We decided to focus our research report—<a href="https://blog.fastforwardlabs.com/2019/04/02/a-guide-to-learning-with-limited-labeled-data.html">Learning
with Limited Labeled Data</a>—on active learning for
deep neural networks, but we were both intrigued and fascinated with meta-learning as an emerging capability. This article is an attempt to throw
some light on the great work that’s been done in this area so far.</p>
<h2 id="introduction">Introduction</h2>
<p>Humans have an innate ability to learn new skills quickly. For example, we can look at one instance of a knife and be able to discriminate all knives from other cutlery items, like spoons and forks. Our ability to learn new skills and adapt to new environments quickly (based on only a few experiences or demonstrations) is not just limited to identifying new objects, learning a new language, or figuring out how to use a new tool;  our capabilities are much more varied. In contrast, machines—especially deep learning algorithms—typically learn quite differently. They require vast amounts of data and compute and may yet struggle to generalize. The reason humans are successful in adapting and learning quickly is that they leverage knowledge acquired from prior experience to solve novel tasks. In a similar fashion, meta-learning leverages previous knowledge acquired from data to solve novel tasks quickly and more efficiently.</p>
<figure><img src="out/figures/1.png" alt="Figure 1: Humans can learn things quickly"><figcaption>Figure 1: Humans can learn things quickly</figcaption></figure>
<h3 id="why-should-we-care%3F">Why should we care?</h3>
<p>An experienced ML practitioner might wonder, isn’t this covered by recent (and much-accoladed) advances in transfer learning? Well, no. Not exactly.
First, supervised learning through deep learning methods requires massive amounts of labeled training data. These datasets are expensive to create, especially when one needs to involve a domain expert. While pre-training is beneficial, these approaches become less effective for domain-specific problems, which still require large amounts of task-specific labeled data to achieve good performance.</p>
<p>In addition, certain real world problems have long-tailed and imbalanced data distributions, which may make it difficult to collect training
examples.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> For instance, in the case of
search engines, perhaps a few keywords are commonly searched for, whereas a vast majority of keywords are rarely searched for. This may result in
poor performance of models/applications based on long-tailed or imbalanced data distributions. The same could be true of recommendation engines;
when there are not enough user reviews or ratings for obscure movies or products, it can
hinder model performance.</p>
<figure><img src="out/figures/2.png" alt="Figure 2: Long-tailed distributions"><figcaption>Figure 2: Long-tailed distributions</figcaption></figure>
<p>Most important, the ability to learn new tasks quickly during model inference is something that conventional machine learning approaches do not attempt. This is what makes meta-learning particularly attractive.</p>
<h3 id="why-now%3F">Why now?</h3>
<p>From a deep learning perspective, meta-learning is particularly exciting and adoptable for three reasons: the ability to learn from a handful of examples, learning or adapting to novel tasks quickly, and the capability to build more generalizable systems. These are also some of the reasons why meta-learning is successful in applications that require data-efficient approaches; for example, robots are tasked with learning new skills in the real world, and are often faced with new environments.</p>
<p>Further, computer vision is one of the major areas in which meta-learning techniques have been explored to solve few-shot learning
problems—including classification, object detection and segmentation, landmark prediction, video synthesis, and others.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>(https://arxiv.org/abs/2004.05439) Additionally, meta-learning has been popular in language modeling tasks, like filling in missing words<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>(https://arxiv.org/abs/1606.04080) and machine translation<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>(https://arxiv.org/abs/1808.08437), and is also being applied to speech recognition tasks, like cross-accent adaptation.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>(https://arxiv.org/abs/2003.01901)</p>
<figure><img src="out/figures/3.png" alt="Figure 3: Applications - object detection, machine translation, missing words"><figcaption>Figure 3: Applications - object detection, machine translation, missing words</figcaption></figure>
<p>As with any other machine learning capability that starts to show promise, there are now libraries and tooling that make meta-learning
more accessible. Although not entirely production-ready, libraries like <a href="https://github.com/tristandeleu/pytorch-meta">torch-meta</a>, <a href="https://github.com/learnables/learn2learn">learn2learn</a> and <a href="https://github.com/google-research/meta-dataset">meta-datasets</a> help handle data, simplify processes when used with popular deep learning frameworks, and help document and benchmark performance on datasets.</p>
<p>The rest of this article, along with its accompanying code, explores meta-learning, provides insight into how it works, and discusses its
implications. We’ll do this using a simple, yet elegant algorithm—Model Agnostic Meta-Learning<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>(https://arxiv.org/pdf/1703.03400.pdf)—applied to a few-shot classification problem, which was proposed a while ago, but
continues to provide a good basis for extension and modification even today.</p>
<h2 id="framing-the-problem">Framing the problem</h2>
<p>What kind of problems can meta-learning help us solve? One of the most popular categories is few-shot learning. In a few-shot learning scenario, we have only a limited number of examples on which to perform supervised learning, and it is important to learn effectively from them. The ability to do so could help relieve the data-gathering burden (which at times may not even be possible).</p>
<p>Let’s say we want to solve a few-shot classification problem, shown in Figure(4) below. Usually the few-shot classification problem is set up as a N-way k-shot problem, where N is the number of classes and k is the number of examples in each class. For example, let’s say we are given an image from each of five different classes (that is, N=5 and k=1) and we are supposed to classify new images as belonging to one of these classes. What can we do? How would one normally model this?</p>
<figure><img src="out/figures/4.png" alt="Figure 4: A few-shot classification (5-way, 1-shot) problem"><figcaption>Figure 4: A few-shot classification (5-way, 1-shot) problem</figcaption></figure>
<p>One way to solve the problem would be to train a neural network model from scratch on the five training images. At a high level, a training step
will look something like Figure(5) below. The neural network model is randomly initialized and receives an image (or images) as input.
It then predicts the output label(s) based on the initial model parameters. The difference between the true label(s) and the predicted label(s)
is measured by a loss function (for example, cross-entropy), which in turn is used to compute the gradients. The gradients are then used to help
calculate new model parameters that best reduce the difference between the “true” and predicted labels. This entire step is known as
backpropagation. After backpropagation, the optimizer updates the model parameters for the model, and all of these steps are repeated for the
rest of the images and/or for some number of epochs, until the loss, evaluated on the train or test data, falls below an acceptable level.</p>
<p><img src="out/figures/5.png" alt="Figure 5: A training step in normal training process"><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></p>
<p>With only five images available for training, chances are that we would likely overfit and perform poorly on the test images. Adding some
regularization or data augmentation may alleviate this problem to some extent, but it will not necessarily solve it. The very nature of a
few-shot problem makes it hard to solve, as there is no prior knowledge of the tasks.</p>
<p>Another possible way to solve the problem could be to use a pre-trained network from another task, and then fine-tune it on the five training
images. However, depending on the problem, this may not always be feasible, especially if the task the network was trained on differs substantially.</p>
<h2 id="solving-the-problem">Solving the problem</h2>
<h3 id="data-set-up">Data set-up</h3>
<p>What meta-learning proposes is to use an end-to-end deep learning algorithm that can learn a representation better suited for few-shot learning.
It is similar to the pre-trained network approach, except that it learns an initialization that serves as a good starting point for the handful of
training data points. In the few-shot classification problem  discussed, we could leverage training data that’s available from other image
classes, for instance, we could look at the training data available and use images from classes like mushrooms, dogs, eyewear, etc. The model
could then build up prior knowledge such that, at inference time, it can quickly acquire task-specific knowledge with only a handful of training
examples.  This way, the model first learns parameters from a training dataset that consists of images from other classes, and then uses those
parameters as prior knowledge to tune them further, based on the limited training set (in this case, the one with five training examples).</p>
<p>Now the question is, how can the model learn a good initial set of parameters which can then be easily adapted to the downstream tasks?
The answer lies in a simple training principle, which was initially proposed by Vinyals et. al.<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>:</p>
<div class="info">
<p>train and test conditions must match.</p>
</div>
<p>The idea is to train a model by showing it only a few examples per class, and then test it against examples from the same classes that have been
held out from the original dataset, much the way it will be tested when presented with only a few training examples from novel classes. Each
training example, in this case, comprises pairs of train and test data points called an <em>episode</em>.</p>
<figure><img src="out/figures/6.png" alt="Figure 6: Meta-learning data setup"><figcaption>Figure 6: Meta-learning data setup<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></figcaption></figure>
<p>This is a departure from the way that data is set up for conventional supervised learning. The training data (also called the meta-training data)
is composed of train and test examples, alternately referred to as the support and query set.</p>
<div class="info">
<p>The number of classes <em>(N)</em> in the support set defines a task as an <em>N</em>-class classification task or <em>N</em>-way task, and the number of labeled
examples in each class <em>(k)</em> corresponds to <em>k</em>-shot, making it an <em>N</em>-way, <em>k</em>-shot learning problem.</p>
</div>
<p>In this case, we have a 5-way, 1-shot learning problem.</p>
<p>Similar to conventional supervised learning, which sets aside validation and test datasets for hyper-parameter tuning and generalization,
meta-learning also has meta-validation and meta-test sets. These are organized in a similar fashion as the meta-training dataset in episodes,
each with support and query sets; the only difference is that the class categories are split into meta-training, validation, and test datasets,
such that the classes do not overlap.</p>
<h3 id="meta-learning-2">Meta-learning</h3>
<p>A meta-learning model should be trained on a variety of tasks, and then optimized further for novel tasks. A task, in this case, is basically a
supervised learning problem (like image classification or regression). The idea is to extract prior information from a set of tasks that allows
efficient learning on new tasks. For our image classification problem, the ideal set-up would include many classes, with at least a few examples
for each. These can then be used as a meta-training set to extract prior information, such that when a new task like the one in the Figure(4)
above comes in, the model can perform it more efficiently.</p>
<p>At a high level, the meta-learning process has two phases: meta-learning and adaptation. In the meta-learning phase, the model learns an initial
set of parameters slowly across tasks; during the adaptation phase, it focuses on quick acquisition of knowledge to learn task-specific
parameters. Since the learning happens at two levels, meta-learning is also known as learning to learn.<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<p>A variety of approaches have been proposed that vary based on how the adaptation portion of the training process performs. These can broadly be classified into three categories: “black-box” or model-based, metric-based, and optimization-based approaches.</p>
<p>“Black-box” (or model-based) approaches simply train an entire neural network, given some training examples in the support set and an initial
set of meta-parameters, and then make predictions on the query set. They approach the problem as supervised learning, although there are
approaches that try to eliminate the need to learn an entire network.<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></p>
<p>Metric-based approaches usually employ non-parametric techniques (for example, <em>k</em>-nearest neighbors) for learning. The core idea is to learn a
feature representation (e.g.,  learning an embedding network that transforms raw inputs into a representation which allows similarity comparison
between the support set and the query set). Thus, performance depends on the chosen similarity metric (like cosine similarity or euclidean
distance).</p>
<p>Finally, optimization-based approaches treat the adaptation part of the process as an optimization problem. This article mainly focuses on one of
the well-known approaches in this category, but before we delve into it, let’s look at how optimization-based learning actually works.</p>
<p>During training, we iterate over datasets of episodes. In meta-training, we start with the first episode, and the meta-learner takes the training
(support) set and produces a learner (or a model) that will take as input the test (query) set and make predictions on it. The meta-learning
objective is based on a loss (for example, cross-entropy) that is derived from the test or query set examples and will backpropagate through these
errors. The parameters of the meta-learner (that is, meta-parameters) are then updated based on these errors to optimize the loss.<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></p>
<p>In the next step, we look at the next episode, train on the support set examples, make predictions on the query set, update meta-parameters, and
repeat. In attempting to learn a meta-learner this way, we are trying to solve the problem of generalization. The examples in the test (or query)
set are not part of the training—so, in a way, the meta-learner is learning to extrapolate.</p>
<figure><img src="out/figures/7.png" alt="Figure 7: Learning to learn"><figcaption>Figure 7: Learning to learn</figcaption></figure>
<h3 id="model-agnostic-meta-learning-(maml)">Model Agnostic Meta-learning (MAML)</h3>
<p>Now that we have a general idea of how meta-learning works, the rest of this article mainly focuses on MAML<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>, which is perhaps one of the best known optimization-based approaches.
While there have been more recent extensions to it, MAML continues to serve as a foundational approach.</p>
<p>The goal of meta-learning is to help the model quickly adapt to or learn on a new task, based on only a few examples. In order for that to happen,
the meta-learning process has to help the model learn from a large number of tasks.  For example, for the image classification problem we’ve
considered, the new task is the one shown in Figure(4), while the large number of tasks could be images from other classes that are utilized for
building a meta-training dataset, as shown in Figure(6).</p>
<p>The key idea in MAML is to establish initial model parameters in the meta-training phase that maximize its performance on the new task. This is
done by updating the initial model parameters with a few gradient steps on the new task. Training the model parameters in this way allows the
model to learn an internal feature representation that is broadly suitable for many tasks—the intuition being that learning an initialization that
is good enough, and then fine-tuning the model slightly, will produce good results.</p>
<p>Imagine we have two neural network models that share the same model architecture:<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> <em>learner</em> for the meta-learning process and <em>adapter</em> for
the adaptation process. Since we have two models to train, we also have two different learning rates associated with them. The MAML algorithm can
then be summarized in the following steps:</p>
<p>&lt;&lt;Team todo: Does the “while not done” below make sense?&gt;&gt;</p>
<div class="info">
<ul>
<li>Step 1: Randomly initialize the learner</li>
<li>Step 2: While not done
<ul>
<li>Step 2.a: Sample a batch of episodes from the meta-training dataset</li>
<li>Step 2.b: Initialize the adapter with the learner’s parameters</li>
<li>Step 2.c: While number of inner training steps is not equal to zero</li>
<li>Step 2.c.1: Train the adapter based on the support set(s) of the batch, compute the loss and the gradients, and update the adapter’s parameters</li>
<li>Step 2.d: Use the updated parameters of the adapter to compute the “meta-loss” based on the query set(s) of the batch</li>
</ul>
</li>
<li>Step 3: Compute the “meta-gradients”, followed by the “meta-parameters” based on the “meta-loss,” and update the learner’s parameters</li>
<li>Step 4: Repeat the entire process from Step (2) for the remaining batches of the meta-training dataset (or for a certain number of epochs) until the learner converges to a good set of “meta-parameters.”</li>
</ul>
</div>
<p>The “meta-loss” indicates how well the model is performing on the task. In effect, the <em>learner</em> is being fine-tuned using a gradient-based
approach for every new task in the batch of episodes. Further, the <em>learner</em> acts as initialization parameters for the <em>adapter</em> so that it can
perform task-specific learning.</p>
<figure><img src="out/figures/8.png" alt="Figure 8: MAML"><figcaption>Figure 8: MAML</figcaption></figure>
<p>During inference, we actually use the meta-trained model (<em>learner</em>) to predict on the meta-test set, except this time—although the meta-trained
model undergoes additional gradient steps to help classify the query set examples—the <em>learner</em> parameters aren’t updated.</p>
<p>As long as the model is trained using gradient descent, the approach does not place any constraints on the model architecture or the loss
function. This characteristic makes it applicable to a wide variety of problems, including regression, classification, and reinforcement learning.
Further, since the approach actually undergoes a few gradient steps for a novel task, it allows the model to perform better on out-of-sample data,
and hence achieves better generalization. This behavior can be attributed to the central assumption of meta-learning: that the tasks are
inherently related and thus data-driven inductive bias can be leveraged to achieve better generalization.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://papers.nips.cc/paper/7278-learning-to-model-the-tail.pdf">Learning to Model the Tail (PDF)</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Meta-learning in Neural Networks: A Survey <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Matching Networks for One-Shot Learning <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Meta-Learning for Low-Resource Neural Machine Translation <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Learning Fast Adaptation on Cross-Accented Speech Recognition <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Model Agnostic Meta-learning for Fast Adaptation of Deep Networks (PDF) <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Adopted from HuggingFace’s blog post, <a href="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#0f06">“From zero to research”</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="https://arxiv.org/abs/1606.04080">Matching Networks for One-Shot Learning</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p> Figure adopted from <a href="https://openreview.net/pdf?id=rJY0-Kcll">Optimization as a Model for Few-Shot Learning (PDF)</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Thrun S., Pratt L. (eds). <a href="https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1">Learning to Learn</a>. Springer, Boston, MA. 1998. <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1">One-shot Learning with Memory-Augmented Neural Networks</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Note that this
differs from a conventional supervised learning set up, in which the objective is based on a loss derived only from the training set, and, of
course, there is no support or query set! <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://arxiv.org/pdf/1703.03400.pdf">Model Agnostic Meta-learning for Fast Adaptation of Deep Networks (PDF)</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>While it is possible to have two duplicate models that can
share parameter tensors in popular deep learning frameworks like PyTorch, libraries like <a href="https://github.com/tristandeleu/pytorch-meta">torch-meta</a>
have extended the existing torch modules to allow storing additional/new parameters. <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

        </div>
      </body>
   </html>
  